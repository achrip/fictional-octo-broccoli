{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: LLM Preparation - WizardLM\n",
        "---"
      ],
      "id": "b3bba276"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "!pip -q install langchain tiktoken chromadb pypdf transformers InstructorEmbedding\n",
        "!pip -q install accelerate bitsandbytes sentencepiece Xformers"
      ],
      "id": "2ce80542",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import torch \n",
        "import transformers\n",
        "from transformers import LlamaTokenizer, LlamaForCasualLM, GenerationConfig, pipeline\n",
        "\n",
        "tokenizer = LlamaTokenizer.from_pretrained(\"TheBloke/wizardLM-7b-HF\")\n",
        "\n",
        "model = LlamaToeknizer.from_pretrained(\"TheBloke/wizardLM-7b-HF\", \n",
        "                                        load_in_8bit=True,\n",
        "                                        device_map='auto', \n",
        "                                        torch_dtype=torch.float16,\n",
        "                                        low_cpu_mem_usage=True\n",
        "                                        )"
      ],
      "id": "165a0339",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.llms import HuggingFacePipeline\n",
        "\n",
        "pipe = pipeline(\n",
        "        \"text-generation\", \n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "        max_length=1024,\n",
        "        temperature=0,\n",
        "        top_p=.95,\n",
        "        repetition_penalty=1.15\n",
        "      )\n",
        "\n",
        "local_llm = HuggingFacePipeline(pipeline=pipe)"
      ],
      "id": "ce9aec80",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# LangChain multi-doc retriever with ChromaDB\n",
        "\n",
        "## Setting up LangChain\n"
      ],
      "id": "3d397cc2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os"
      ],
      "id": "6acfee4e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.vectorstores import Chroma\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain.document_loaders import TextLoader\n",
        "from langchain.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders import DirectoryLoader\n",
        "\n",
        "from InstructorEmbedding import INSTRUCTOR\n",
        "from langchain.embeddings import HuggingFaceInstructEmbeddings"
      ],
      "id": "09723574",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load multiple documents and process\n"
      ],
      "id": "ff32872d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Load and process the text files\n",
        "# loader = TextLoader('single_text_file.txt')\n",
        "loader = DirectoryLoader('./assets/ncvs_documents', glob=\"./*.pdf\", loader_cls=PyPDFLoader)\n",
        "\n",
        "documents = loader.load()"
      ],
      "id": "66b83f26",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#splitting the text into\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "texts = text_splitter.split_documents(documents)"
      ],
      "id": "8b8bafa5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## HuggingFace Instructor Embeddings\n"
      ],
      "id": "1b848cc4"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from langchain.embeddings import HuggingFaceInstructEmbeddings\n",
        "\n",
        "instructor_embeddings = HuggingFaceInstructEmbeddings(model_name=\"hkunlp/instructor-xl\",\n",
        "                                                      model_kwargs={\"device\": \"cuda\"})"
      ],
      "id": "73925b56",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Database\n"
      ],
      "id": "1e338f8b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Embed and store the texts\n",
        "# Supplying a persist_directory will store the embeddings on disk\n",
        "persist_directory = 'db'\n",
        "\n",
        "## Here is the nmew embeddings being used\n",
        "embedding = instructor_embeddings\n",
        "\n",
        "vectordb = Chroma.from_documents(documents=texts,\n",
        "                                 embedding=embedding,\n",
        "                                 persist_directory=persist_directory)"
      ],
      "id": "5043e531",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make a retriever\n"
      ],
      "id": "b2d2b393"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "retriever = vectordb.as_retriever(search_kwargs={\"k\": 3})"
      ],
      "id": "88169946",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Make a chain\n"
      ],
      "id": "50745462"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# create the chain to answer questions\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=local_llm,\n",
        "                                  chain_type=\"stuff\",\n",
        "                                  retriever=retriever,\n",
        "                                  return_source_documents=True)"
      ],
      "id": "8c7d81ac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Cite sources\n"
      ],
      "id": "a2c85e3a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import textwrap\n",
        "\n",
        "def wrap_text_preserve_newlines(text, width=110):\n",
        "    # Split the input text into lines based on newline characters\n",
        "    lines = text.split('\\n')\n",
        "\n",
        "    # Wrap each line individually\n",
        "    wrapped_lines = [textwrap.fill(line, width=width) for line in lines]\n",
        "\n",
        "    # Join the wrapped lines back together using newline characters\n",
        "    wrapped_text = '\\n'.join(wrapped_lines)\n",
        "\n",
        "    return wrapped_text\n",
        "\n",
        "def process_llm_response(llm_response):\n",
        "    print(wrap_text_preserve_newlines(llm_response['result']))\n",
        "    print('\\n\\nSources:')\n",
        "    for source in llm_response[\"source_documents\"]:\n",
        "        print(source.metadata['source'])"
      ],
      "id": "0e7852d5",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "asr",
      "language": "python",
      "name": "asr"
    },
    "jupytext": {
      "formats": "qmd:quarto,ipynb",
      "text_representation": {
        "extension": ".qmd",
        "format_name": "quarto",
        "format_version": "1.0",
        "jupytext_version": "1.16.2"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}